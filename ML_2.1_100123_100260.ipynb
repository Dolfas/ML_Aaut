{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML PROBLEM 4.1**\n",
    "\n",
    "Alexandre Reis nº100123\n",
    "\n",
    "Rodolfo Amorim nº100260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import linear_model \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import *\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the data from the files given**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.abspath('X_train_regression1.npy'))\n",
    "y_train = np.load(os.path.abspath('y_train_regression1.npy'))\n",
    "X_test  = np.load(os.path.abspath('X_test_regression1.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Centering of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean  = np.mean(X_train,axis=0) \n",
    "y_train_mean  = np.mean(y_train,axis=0)\n",
    "\n",
    "X_train_prime = X_train.copy()\n",
    "y_train_prime = y_train.copy()\n",
    "\n",
    "\n",
    "for i in range(10): #the centering is done with the average of each feature\n",
    "    X_train_prime[:,i] = X_train_prime[:,i] -  X_train_mean[i]\n",
    "\n",
    "# y_train_prime is a one dimensional array, and as such we do not include it in the for loop \n",
    "y_train_prime = y_train_prime - y_train_mean \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application of the LASSO Regularization**\n",
    "In this case, the objective of using the Lasso Regularization is to create a sparse Beta matrix, which will dictate which of the 10 features of matrix x are important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LassoModelCV = LassoCV(alphas=np.arange(0,10,0.01),cv=4,fit_intercept=False,random_state=0).fit(X_train_prime, y_train_prime) \n",
    "#LassoCv is used to determine the best alpha hyperparameter for the lasso model\n",
    "#we are using cv=8 because it is the one wich proves to give the least mean squared error (this is proven in the final part of this document)\n",
    "#as the pre-processed data is already centered, fit-intercept is set to false\n",
    "beta_hat_lasso  = LassoModelCV.coef_ # Beta Coefficients from the LASSO Regularization\n",
    "\n",
    "\n",
    "# Removal of the unnecessary columns from the X data - this coluns are the pnes corresponding to features with zero value in the Lasso's sparce beta matrix \n",
    "X_train_prime_lasso_ridge= X_train_prime.copy()\n",
    "X_test_lasso_ridge = X_test.copy()\n",
    "for i in range(np.shape(beta_hat_lasso)[0]-1, -1, -1):\n",
    "    if beta_hat_lasso[i]==0:\n",
    "        X_train_prime_lasso_ridge = np.delete(X_train_prime_lasso_ridge, i, 1)\n",
    "        X_test_lasso_ridge = np.delete(X_test_lasso_ridge, i, 1)\n",
    "        X_train_mean = np.delete(X_train_mean, i, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplication of the Ridge Regularization to the LASSO Regularization Coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RidgeModel = linear_model.RidgeCV(alphas=np.arange(0.01,10,0.01), fit_intercept=False, scoring=\"neg_mean_squared_error\", cv=4).fit(X_train_prime_lasso_ridge, y_train_prime)\n",
    "\n",
    "beta_hat_ridge= RidgeModel.coef_  # Beta Coefficients from the application of the Ridge Regularization to the Coefficients from the LASSO Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of the Beta Coefficients to make predictions on Uncentered data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_zero = y_train_mean - np.matmul((X_train_mean),np.transpose(beta_hat_ridge))\n",
    "m = [beta_zero[0]]\n",
    "for k in range(((beta_hat_ridge).shape[1])):\n",
    "        m.append(beta_hat_ridge[:,k][0])\n",
    "beta_hat_lasso_ridge = np.array(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction for the Test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = np.hstack((np.ones((1000,1)),X_test_lasso_ridge))\n",
    "prediction = np.matmul(X_test_final,beta_hat_lasso_ridge)\n",
    "prediciton = prediction.reshape(1000,1)\n",
    "np.save('prediction_100123_100260_4.1',prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appendix**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that the cv used in LassoCv model is the better with regards to the mean squared error and to get to know the optimal range for alpha the following analysis was made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_otimization_1(matrix_cv_alpha_score):\n",
    "    i = np.argmax(matrix_cv_alpha_score[:,2])\n",
    "    print(\"The values that optimize the mean squared error are:\")\n",
    "    print(\"Alpha=\" + str(matrix_cv_alpha_score[i,1]))\n",
    "    print(\"cv=\" + str(matrix_cv_alpha_score[i,0]))\n",
    "    print(\"For a MSE=\" + str(matrix_cv_alpha_score[i,2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LassoCv_otimization(x,y, fit_intercept_bool):\n",
    "    LassoOptimization_model_parameters = np.empty([0,3])\n",
    "    #we are going to test every possible value for k-fold cross validation \n",
    "    for cv_value in np.arange(2,15,1):\n",
    "        LassoOptimization_modelcv = LassoCV(alphas=np.arange(0,10,0.01),cv=cv_value,fit_intercept = fit_intercept_bool, random_state=0).fit(x, y)\n",
    "      \n",
    "\n",
    "        cv_results = cross_validate(LassoOptimization_modelcv, x, y,scoring='neg_mean_squared_error', cv=LeaveOneOut())\n",
    "\n",
    "        neg_mean_squared_errors = cv_results['test_score']\n",
    "        mean_neg_mean_squared_error = np.mean(neg_mean_squared_errors)\n",
    "\n",
    "        LassoOptimization_model_parameters = np.vstack((LassoOptimization_model_parameters, np.array([cv_value , LassoOptimization_model.alpha_, mean_neg_mean_squared_error])))\n",
    "    MSE_otimization_1(LassoOptimization_model_parameters)\n",
    "    return(LassoOptimization_model_parameters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimization for x and y centered train models - Lasso\n",
      "The values that optimize the mean squared error are:\n",
      "Alpha=0.05\n",
      "cv=4.0\n",
      "For a MSE=-2.038957005902529\n"
     ]
    }
   ],
   "source": [
    "print(\"Otimization for x and y centered train models - Lasso\")\n",
    "Lasso_Optimization_results = LassoCv_otimization(X_train_prime, y_train_prime, False)\n",
    "Lasso_Optimization_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimization was done with and without the data previously centered and we concluded that the MSE is closer to zero with centered data\n",
    "\n",
    "Otimization for x and y centered train models - Lasso\n",
    "The values that optimize the mean squared error are:\n",
    "Alpha=0.05\n",
    "cv=4.0\n",
    "For a MSE=-2.038957005902529"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that the cv used in RidgeCV model is the better with regards to the mean squared error, to get to know the optimal alpha for Ridge's model and to decide wether to center or not the data, the following analysis was made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_otimization_2(matrix_cv_alpha_score):\n",
    "    i = np.argmax(matrix_cv_alpha_score[:,3])\n",
    "    print(\"The values that optimize the mean squared error are:\")\n",
    "    print(\"Alpha=\" + str(matrix_cv_alpha_score[i,1]))\n",
    "    print(\"cv=\" + str(matrix_cv_alpha_score[i,0]))\n",
    "    print(\"For a MSE=\" + str(matrix_cv_alpha_score[i,3]))\n",
    "    print(\"MSE calculated via ridgecv - with fewer coluns in x=\" + str(matrix_cv_alpha_score[i,2]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RidgeCv_otimization(x,y, fit_intercept_bool):\n",
    "    RidgeOptimization_model_parameters = np.empty([0,4])\n",
    "    #we are going to test every possible value for k-fold cross validation including k=15 which is equivalent to the efficient Leave One Out method\n",
    "\n",
    "    for cv_value in np.arange(2,15,1):\n",
    "        RidgeOptimization_model = linear_model.RidgeCV(alphas=np.arange(0.01,10,0.01), fit_intercept=fit_intercept_bool, scoring=\"neg_mean_squared_error\", cv=cv_value).fit(x, y)\n",
    "        \n",
    "        cv_results = cross_validate(RidgeOptimization_model, x, y,scoring='neg_mean_squared_error', cv=LeaveOneOut())\n",
    "\n",
    "        neg_mean_squared_errors = cv_results['test_score']\n",
    "        mean_neg_mean_squared_error = np.mean(neg_mean_squared_errors)\n",
    "\n",
    "        RidgeOptimization_model_parameters = np.vstack((RidgeOptimization_model_parameters, np.array([cv_value , RidgeOptimization_model.alpha_, RidgeOptimization_model.best_score_, mean_neg_mean_squared_error])))\n",
    "    MSE_otimization_2(RidgeOptimization_model_parameters)\n",
    "    return(RidgeOptimization_model_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_ridge(x, y):\n",
    "    Model_lasso = LassoCV(alphas=np.arange(0,10,0.05),cv=4,fit_intercept = False, random_state=0).fit(x, y)\n",
    "     \n",
    "    beta_hat_lasso_Model  = Model_lasso.coef_ # Beta Coefficients from the LASSO Regularization\n",
    "\n",
    "\n",
    "    # Removal of the unnecessary columns from the X data - this coluns are the pnes corresponding to features with zero value in the Lasso's sparce beta matrix \n",
    "    X_train_prime_lasso_ridge_appendix= x.copy()\n",
    "    for i in range(np.shape(beta_hat_lasso_Model)[0]-1, -1, -1):\n",
    "        if beta_hat_lasso_Model[i]==0:\n",
    "            X_train_prime_lasso_ridge_appendix = np.delete(X_train_prime_lasso_ridge_appendix, i, 1)\n",
    "\n",
    "    return X_train_prime_lasso_ridge_appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimization for x and y centered train models, with some x coluns/features deleted, based on Lasso's model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\Aprendizagem Automática\\Labs2\\ML_Aaut\\ML_2.1_100123_100260.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/Aprendizagem%20Autom%C3%A1tica/Labs2/ML_Aaut/ML_2.1_100123_100260.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#new x variable that only has coluns for features that Lasso's regularization does not make nul in beta^ matrix\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/Aprendizagem%20Autom%C3%A1tica/Labs2/ML_Aaut/ML_2.1_100123_100260.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X_train_prime_lasso_ridge_appendix \u001b[39m=\u001b[39m lasso_ridge(X_train_prime, y_train_prime) \u001b[39m#lasso_alpha was defined when optimizing the lasso pareameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/Aprendizagem%20Autom%C3%A1tica/Labs2/ML_Aaut/ML_2.1_100123_100260.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m RidgeCv_otimization(X_train_prime_lasso_ridge_appendix, y_train_prime, \u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\Aprendizagem Automática\\Labs2\\ML_Aaut\\ML_2.1_100123_100260.ipynb Cell 25\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/Aprendizagem%20Autom%C3%A1tica/Labs2/ML_Aaut/ML_2.1_100123_100260.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m cv_value \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m2\u001b[39m,\u001b[39m15\u001b[39m,\u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/Aprendizagem%20Autom%C3%A1tica/Labs2/ML_Aaut/ML_2.1_100123_100260.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     RidgeOptimization_model \u001b[39m=\u001b[39m linear_model\u001b[39m.\u001b[39mRidgeCV(alphas\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marange(\u001b[39m0.01\u001b[39m,\u001b[39m10\u001b[39m,\u001b[39m0.01\u001b[39m), fit_intercept\u001b[39m=\u001b[39mfit_intercept_bool, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m\"\u001b[39m, cv\u001b[39m=\u001b[39mcv_value)\u001b[39m.\u001b[39mfit(x, y)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/Aprendizagem%20Autom%C3%A1tica/Labs2/ML_Aaut/ML_2.1_100123_100260.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     cv_results \u001b[39m=\u001b[39m cross_validate(RidgeOptimization_model, x, y,scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mneg_mean_squared_error\u001b[39;49m\u001b[39m'\u001b[39;49m, cv\u001b[39m=\u001b[39;49mLeaveOneOut())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/Aprendizagem%20Autom%C3%A1tica/Labs2/ML_Aaut/ML_2.1_100123_100260.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     neg_mean_squared_errors \u001b[39m=\u001b[39m cv_results[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/Aprendizagem%20Autom%C3%A1tica/Labs2/ML_Aaut/ML_2.1_100123_100260.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     mean_neg_mean_squared_error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(neg_mean_squared_errors)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m         clone(estimator),\n\u001b[0;32m    312\u001b[0m         X,\n\u001b[0;32m    313\u001b[0m         y,\n\u001b[0;32m    314\u001b[0m         scorers,\n\u001b[0;32m    315\u001b[0m         train,\n\u001b[0;32m    316\u001b[0m         test,\n\u001b[0;32m    317\u001b[0m         verbose,\n\u001b[0;32m    318\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    319\u001b[0m         fit_params,\n\u001b[0;32m    320\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    321\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    322\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    323\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    324\u001b[0m     )\n\u001b[0;32m    325\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m indices\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_ridge.py:2369\u001b[0m, in \u001b[0;36mRidgeCV.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2339\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   2340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2341\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit Ridge regression model with cv.\u001b[39;00m\n\u001b[0;32m   2342\u001b[0m \n\u001b[0;32m   2343\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2367\u001b[0m \u001b[39m    the validation score.\u001b[39;00m\n\u001b[0;32m   2368\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2369\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m   2370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_ridge.py:2194\u001b[0m, in \u001b[0;36m_BaseRidgeCV.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2184\u001b[0m model \u001b[39m=\u001b[39m RidgeClassifier \u001b[39mif\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m) \u001b[39melse\u001b[39;00m Ridge\n\u001b[0;32m   2185\u001b[0m gs \u001b[39m=\u001b[39m GridSearchCV(\n\u001b[0;32m   2186\u001b[0m     model(\n\u001b[0;32m   2187\u001b[0m         fit_intercept\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2192\u001b[0m     scoring\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring,\n\u001b[0;32m   2193\u001b[0m )\n\u001b[1;32m-> 2194\u001b[0m gs\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m   2195\u001b[0m estimator \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39mbest_estimator_\n\u001b[0;32m   2196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha_ \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39malpha\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_ridge.py:1131\u001b[0m, in \u001b[0;36mRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1122\u001b[0m _accept_sparse \u001b[39m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[39m.\u001b[39missparse(X), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver)\n\u001b[0;32m   1123\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m   1124\u001b[0m     X,\n\u001b[0;32m   1125\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     y_numeric\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1130\u001b[0m )\n\u001b[1;32m-> 1131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_ridge.py:883\u001b[0m, in \u001b[0;36m_BaseRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m         \u001b[39m# for dense matrices or when intercept is set to 0\u001b[39;00m\n\u001b[0;32m    881\u001b[0m         params \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 883\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _ridge_regression(\n\u001b[0;32m    884\u001b[0m         X,\n\u001b[0;32m    885\u001b[0m         y,\n\u001b[0;32m    886\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[0;32m    887\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    888\u001b[0m         max_iter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter,\n\u001b[0;32m    889\u001b[0m         tol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol,\n\u001b[0;32m    890\u001b[0m         solver\u001b[39m=\u001b[39msolver,\n\u001b[0;32m    891\u001b[0m         positive\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive,\n\u001b[0;32m    892\u001b[0m         random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state,\n\u001b[0;32m    893\u001b[0m         return_n_iter\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    894\u001b[0m         return_intercept\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    895\u001b[0m         check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    896\u001b[0m         fit_intercept\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept,\n\u001b[0;32m    897\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    898\u001b[0m     )\n\u001b[0;32m    899\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_intercept(X_offset, y_offset, X_scale)\n\u001b[0;32m    901\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_ridge.py:693\u001b[0m, in \u001b[0;36m_ridge_regression\u001b[1;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, positive, random_state, return_n_iter, return_intercept, X_scale, X_offset, check_input, fit_intercept)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 693\u001b[0m         coef \u001b[39m=\u001b[39m _solve_cholesky(X, y, alpha)\n\u001b[0;32m    694\u001b[0m     \u001b[39mexcept\u001b[39;00m linalg\u001b[39m.\u001b[39mLinAlgError:\n\u001b[0;32m    695\u001b[0m         \u001b[39m# use SVD solver if matrix is singular\u001b[39;00m\n\u001b[0;32m    696\u001b[0m         solver \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msvd\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_ridge.py:200\u001b[0m, in \u001b[0;36m_solve_cholesky\u001b[1;34m(X, y, alpha)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m one_alpha:\n\u001b[0;32m    199\u001b[0m     A\u001b[39m.\u001b[39mflat[:: n_features \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m alpha[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m linalg\u001b[39m.\u001b[39;49msolve(A, Xy, assume_a\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpos\u001b[39;49m\u001b[39m\"\u001b[39;49m, overwrite_a\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mT\n\u001b[0;32m    201\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     coefs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty([n_targets, n_features], dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\linalg\\_basic.py:255\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(a, b, lower, overwrite_a, overwrite_b, check_finite, assume_a, transposed)\u001b[0m\n\u001b[0;32m    251\u001b[0m     lu, x, info \u001b[39m=\u001b[39m posv(a1, b1, lower\u001b[39m=\u001b[39mlower,\n\u001b[0;32m    252\u001b[0m                        overwrite_a\u001b[39m=\u001b[39moverwrite_a,\n\u001b[0;32m    253\u001b[0m                        overwrite_b\u001b[39m=\u001b[39moverwrite_b)\n\u001b[0;32m    254\u001b[0m     _solve_check(n, info)\n\u001b[1;32m--> 255\u001b[0m     rcond, info \u001b[39m=\u001b[39m pocon(lu, anorm)\n\u001b[0;32m    257\u001b[0m _solve_check(n, info, lamch, rcond)\n\u001b[0;32m    259\u001b[0m \u001b[39mif\u001b[39;00m b_is_1D:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print(\"Otimization for x and y original train models\")\n",
    "#RidgeCv_otimization(X_train, y_train, True)\n",
    "\n",
    "print(\"Otimization for x and y centered train models\")\n",
    "RidgeCv_otimization(X_train_prime, y_train_prime, False)\n",
    "\n",
    "print(\"Otimization for x and y centered train models, with some x coluns/features deleted, based on Lasso's model\")\n",
    "\n",
    "#new x variable that only has coluns for features that Lasso's regularization does not make nul in beta^ matrix\n",
    "X_train_prime_lasso_ridge_appendix = lasso_ridge(X_train_prime, y_train_prime) #lasso_alpha was defined when optimizing the lasso pareameters\n",
    "\n",
    "RidgeCv_otimization(X_train_prime_lasso_ridge_appendix, y_train_prime, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otimization for x and y centered train models\n",
    "The values that optimize the mean squared error are:\n",
    "Alpha=2.1599999999999997\n",
    "cv=4.0\n",
    "For a MSE=-2.357425118123902\n",
    "MSE calculated via ridgecv - with fewer coluns in x=-2.6244248458581385\n",
    "\n",
    "Otimization for x and y centered train models, with some x coluns/features deleted, based on Lasso's model\n",
    "The values that optimize the mean squared error are:\n",
    "Alpha=0.51\n",
    "cv=4.0\n",
    "For a MSE=-1.4472918298246262\n",
    "MSE calculated via ridgecv - with fewer coluns in x=-2.7061605786341976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "\n",
    "The  best opion is a lasso with cv=4 and alpha=0,05 followed by a ridge with alpha = 0,51 and cv=4 - MSE=-1.4472918298246262\n",
    "\n",
    "The second best option is a simple lasso with cv=4 and aplha=0,05 - MSE=-2.038957005902529\n",
    "\n",
    "The thir best option is Ridge model with centered x and y with Alpha=1.31 cv=Leave one out - MSE=-2.167693140920147\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
