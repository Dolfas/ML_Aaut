{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML PROBLEM 4.1**\n",
    "\n",
    "Alexandre Reis nº100123\n",
    "\n",
    "Rodolfo Amorim nº100260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import linear_model \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import *\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the data from the files given**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.abspath('X_train_regression1.npy'))\n",
    "y_train = np.load(os.path.abspath('y_train_regression1.npy'))\n",
    "X_test  = np.load(os.path.abspath('X_test_regression1.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Centering of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean  = np.mean(X_train,axis=0) \n",
    "y_train_mean  = np.mean(y_train,axis=0)\n",
    "\n",
    "X_train_prime = X_train.copy()\n",
    "y_train_prime = y_train.copy()\n",
    "\n",
    "\n",
    "for i in range(10): #the centering is done with the average of each feature\n",
    "    X_train_prime[:,i] = X_train_prime[:,i] -  X_train_mean[i]\n",
    "\n",
    "# y_train_prime is a one dimensional array, and as such we do not include it in the for loop \n",
    "y_train_prime = y_train_prime - y_train_mean \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application of the LASSO Regularization**\n",
    "In this case, the objective of using the Lasso Regularization is to create a sparse Beta matrix, which will dictate which of the 10 features of matrix x are important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LassoModelCV = LassoCV(alphas=np.arange(0,10,0.01),cv=8,fit_intercept=False,random_state=0).fit(X_train_prime, y_train_prime) \n",
    "#LassoCv is used to determine the best alpha hyperparameter for the lasso model\n",
    "#we are using cv=8 because it is the one wich proves to give the least mean squared error (this is proven in the final part of this document)\n",
    "#as the pre-processed data is already centered, fit-intercept is set to false\n",
    "LassoModel = Lasso(LassoModelCV.alpha_).fit(X_train_prime, y_train_prime) \n",
    "beta_hat_lasso  = LassoModel.coef_ # Beta Coefficients from the LASSO Regularization\n",
    "\n",
    "\n",
    "# Removal of the unnecessary columns from the X data - this coluns are the pnes corresponding to features with zero value in the Lasso's sparce beta matrix \n",
    "X_train_prime_lasso_ridge= X_train_prime.copy()\n",
    "X_test_lasso_ridge = X_test.copy()\n",
    "for i in range(np.shape(beta_hat_lasso)[0]-1, -1, -1):\n",
    "    if beta_hat_lasso[i]==0:\n",
    "        X_train_prime_lasso_ridge = np.delete(X_train_prime_lasso_ridge, i, 1)\n",
    "        X_test_lasso_ridge = np.delete(X_test_lasso_ridge, i, 1)\n",
    "        X_train_mean = np.delete(X_train_mean, i, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplication of the Ridge Regularization to the LASSO Regularization Coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RidgeModel = linear_model.RidgeCV(alphas=np.arange(0.01,10,0.01), fit_intercept=False, scoring=\"neg_mean_squared_error\", cv=None).fit(X_train_prime_lasso_ridge, y_train_prime)\n",
    "\n",
    "beta_hat_ridge= RidgeModel.coef_  # Beta Coefficients from the application of the Ridge Regularization to the Coefficients from the LASSO Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of the Beta Coefficients to make predictions on Uncentered data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_zero = y_train_mean - np.matmul((X_train_mean),np.transpose(beta_hat_ridge))\n",
    "m = [beta_zero[0]]\n",
    "for k in range(((beta_hat_ridge).shape[1])):\n",
    "        m.append(beta_hat_ridge[:,k][0])\n",
    "beta_hat_lasso_ridge = np.array(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction for the Test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = np.hstack((np.ones((1000,1)),X_test_lasso_ridge))\n",
    "prediction = np.matmul(X_test_final,beta_hat_lasso_ridge)\n",
    "prediciton = prediction.reshape(1000,1)\n",
    "np.save('prediction_100123_100260_4.1',prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appendix**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that the cv used in LassoCv model is the better with regards to the mean squared error and to get to know the optimal range for alpha the following analysis was made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_otimization_1(matrix_cv_alpha_score):\n",
    "    i = np.argmax(matrix_cv_alpha_score[:,2])\n",
    "    print(\"The values that optimize the mean squared error are:\")\n",
    "    print(\"Alpha=\" + str(matrix_cv_alpha_score[i,1]))\n",
    "    print(\"cv=\" + str(matrix_cv_alpha_score[i,0]))\n",
    "    print(\"For a MSE=\" + str(matrix_cv_alpha_score[i,2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LassoCv_otimization(x,y, fit_intercept_bool):\n",
    "    LassoOptimization_model_parameters = np.empty([0,3])\n",
    "    #we are going to test every possible value for k-fold cross validation \n",
    "    for cv_value in np.arange(2,15,1):\n",
    "        LassoOptimization_model = LassoCV(alphas=np.arange(0,10,0.01),cv=cv_value,fit_intercept = False, random_state=0).fit(x, y)\n",
    "\n",
    "        cv_results = cross_validate(LassoOptimization_model, X_train_prime, y_train_prime,scoring='neg_mean_squared_error', cv=LeaveOneOut())\n",
    "\n",
    "        neg_mean_squared_errors = cv_results['test_score']\n",
    "        mean_neg_mean_squared_error = np.mean(neg_mean_squared_errors)\n",
    "\n",
    "        LassoOptimization_model_parameters = np.vstack((LassoOptimization_model_parameters, np.array([cv_value , LassoOptimization_model.alpha_, mean_neg_mean_squared_error])))\n",
    "    MSE_otimization_1(LassoOptimization_model_parameters)\n",
    "    return(LassoOptimization_model_parameters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimization for x and y centered train models - Lasso\n",
      "The values that optimize the mean squared error are:\n",
      "Alpha=0.05\n",
      "cv=4.0\n",
      "For a MSE=-2.038957005902529\n"
     ]
    }
   ],
   "source": [
    "print(\"Otimization for x and y centered train models - Lasso\")\n",
    "Lasso_Optimization_results = LassoCv_otimization(X_train_prime, y_train_prime, False)\n",
    "Lasso_Optimization_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otimization for x and y centered train models - Lasso\n",
    "The values that optimize the mean squared error are:\n",
    "Alpha=0.05\n",
    "cv=4.0\n",
    "For a MSE=-2.038957005902529"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that the cv used in RidgeCV model is the better with regards to the mean squared error, to get to know the optimal alpha for Ridge's model and to decide wether to center or not the data, the following analysis was made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_otimization_2(matrix_cv_alpha_score):\n",
    "    i = np.argmax(matrix_cv_alpha_score[:,3])\n",
    "    print(\"The values that optimize the mean squared error are:\")\n",
    "    print(\"Alpha=\" + str(matrix_cv_alpha_score[i,1]))\n",
    "    print(\"cv=\" + str(matrix_cv_alpha_score[i,0]))\n",
    "    print(\"For a MSE=\" + str(matrix_cv_alpha_score[i,3]))\n",
    "    print(\"MSE calculated via ridgecv - with fewer coluns in x=\" + str(matrix_cv_alpha_score[i,2]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RidgeCv_otimization(x,y, fit_intercept_bool):\n",
    "    RidgeOptimization_model_parameters = np.empty([0,4])\n",
    "    #we are going to test every possible value for k-fold cross validation including k=15 which is equivalent to the efficient Leave One Out method\n",
    "\n",
    "    for cv_value in np.arange(2,15,1):\n",
    "        RidgeOptimization_model = linear_model.RidgeCV(alphas=np.arange(0.01,10,0.05), fit_intercept=fit_intercept_bool, scoring=\"neg_mean_squared_error\", cv=cv_value).fit(x, y)\n",
    "        cv_results = cross_validate(RidgeOptimization_model, X_train_prime, y_train_prime,scoring='neg_mean_squared_error', cv=LeaveOneOut())\n",
    "\n",
    "        neg_mean_squared_errors = cv_results['test_score']\n",
    "        mean_neg_mean_squared_error = np.mean(neg_mean_squared_errors)\n",
    "\n",
    "        RidgeOptimization_model_parameters = np.vstack((RidgeOptimization_model_parameters, np.array([cv_value , RidgeOptimization_model.alpha_, RidgeOptimization_model.best_score_, mean_neg_mean_squared_error])))\n",
    "    MSE_otimization_2(RidgeOptimization_model_parameters)\n",
    "    return(RidgeOptimization_model_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_ridge(x, y, lasso_alpha):\n",
    "   \n",
    "    Model_lasso = Lasso(lasso_alpha).fit(X_train_prime, y_train_prime) \n",
    "    beta_hat_lasso_Model  = Model_lasso.coef_ # Beta Coefficients from the LASSO Regularization\n",
    "\n",
    "\n",
    "    # Removal of the unnecessary columns from the X data - this coluns are the pnes corresponding to features with zero value in the Lasso's sparce beta matrix \n",
    "    X_train_prime_lasso_ridge_appendix= x.copy()\n",
    "    for i in range(np.shape(beta_hat_lasso_Model)[0]-1, -1, -1):\n",
    "        if beta_hat_lasso_Model[i]==0:\n",
    "            X_train_prime_lasso_ridge_appendix = np.delete(X_train_prime_lasso_ridge_appendix, i, 1)\n",
    "\n",
    "    return X_train_prime_lasso_ridge_appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimization for x and y centered train models, with some x coluns/features deleted, based on Lasso's model\n",
      "The values that optimize the mean squared error are:\n",
      "Alpha=0.51\n",
      "cv=4.0\n",
      "For a MSE=-2.357425118123902\n",
      "MSE calculated via ridgecv - with fewer coluns in x=-2.7061605786341976\n"
     ]
    }
   ],
   "source": [
    "#print(\"Otimization for x and y original train models\")\n",
    "#RidgeCv_otimization(X_train, y_train, True)\n",
    "\n",
    "print(\"Otimization for x and y centered train models\")\n",
    "RidgeCv_otimization(X_train_prime, y_train_prime, False)\n",
    "\n",
    "print(\"Otimization for x and y centered train models, with some x coluns/features deleted, based on Lasso's model\")\n",
    "\n",
    "#new x variable that only has coluns for features that Lasso's regularization does not make nul in beta^ matrix\n",
    "X_train_prime_lasso_ridge_appendix = lasso_ridge(X_train_prime, y_train_prime, 0.05) #lasso_alpha was defined when optimizing the lasso pareameters\n",
    "\n",
    "RidgeCv_otimization(X_train_prime_lasso_ridge_appendix, y_train_prime, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otimization for x and y centered train models\n",
    "The values that optimize the mean squared error are:\n",
    "Alpha=2.1599999999999997\n",
    "cv=4.0\n",
    "For a MSE=-2.6244248458581385\n",
    "MSE for original data=-2.357425118123902\n",
    "\n",
    "Otimization for x and y centered train models, with some x coluns/features deleted, based on Lasso's model\n",
    "The values that optimize the mean squared error are:\n",
    "Alpha=0.51\n",
    "cv=4.0\n",
    "For a MSE=-2.357425118123902\n",
    "MSE calculated via ridgecv - with fewer coluns in x=-2.7061605786341976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.837412684458413\n",
      "ALpha do ridge 1.31\n",
      "Best score do ridge -2.167693140920147\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.RidgeCV(alphas=np.arange(0.01,10,0.01), fit_intercept=False, cv=None).fit(X_train_prime, y_train_prime)\n",
    "cv_results = cross_validate(reg, X_train_prime, y_train_prime,scoring='neg_mean_squared_error', cv=LeaveOneOut())\n",
    "neg_mean_squared_errors = cv_results['test_score']\n",
    "mean_neg_mean_squared_error = np.mean(neg_mean_squared_errors)\n",
    "print(mean_neg_mean_squared_error)\n",
    "\n",
    "print(\"ALpha do ridge\", reg.alpha_)\n",
    "print(\"Best score do ridge\",reg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results state that the option with the least Mean Squared Error is the Ridge Regularization apllied to the LASSO Regularization Coefficients, with a cv=3 and alpha=0.47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
